# -*- coding: utf-8 -*-
"""01 - QML in SCM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRNu559A47ny_07WldCwJpkVvgD8H707

# Libraries
"""

# !pip install tensorflow-addons
# !pip3 install kagglehub

# #Installing Qiskit Packages
# !pip install qiskit
# !pip install qiskit_machine_learning
# !pip install qiskit_algorithms
# !pip install qiskit_ibm_runtime

from pathlib import Path
#import tensorflow as tf
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time
import requests
import seaborn as sns

#Importing Libraries
from sklearn.svm import SVC

from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#from qiskit.utils import algorithm_globals
from qiskit.circuit.library import PauliFeatureMap, ZZFeatureMap
from qiskit_algorithms.state_fidelities import ComputeUncompute
from qiskit_machine_learning.kernels import FidelityQuantumKernel

from qiskit_ibm_runtime import QiskitRuntimeService
from qiskit_machine_learning.algorithms.classifiers import QSVC
from qiskit.circuit import QuantumCircuit
from qiskit import transpile
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import COBYLA
from qiskit.primitives import BaseSampler
#from qiskit.primitives import Sampler
#from qiskit_ibm_runtime import QiskitRuntimeService, Sampler, SamplerV2
from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler

from sklearn.pipeline import make_pipeline
from qiskit_machine_learning.state_fidelities import ComputeUncompute
from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes
from qiskit_machine_learning.kernels import FidelityQuantumKernel
from qiskit import transpile
from sklearn.svm import SVC
import pandas as pd
import time

np.random.seed(42)
#algorithm_globals.random_seed = 123

"""# Dataset

[ClaMp](https://www.kaggle.com/code/ssmohanty/dimensionality-reduction-techniques)
"""

#import kagglehub

## Download latest version
#path = kagglehub.dataset_download("saurabhshahane/classification-of-malwares")

#print("Path to dataset files:", path)

import pandas as pd
import os

# Define the dataset path
dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"

# List files in the directory to find the CSV file
files = os.listdir(dataset_path)
csv_files = [f for f in files if f.endswith('.csv')]

# Load the first CSV file (assuming there's only one)
if csv_files:
    df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
    print("CSV found and send to df")  # Display first few rows
else:
    print("No CSV file found in the dataset directory.")

df.head()

target = 'class'

import pandas as pd

def create_balanced_sample(df, target_column='class', num_samples=1000):
    # Ensure that num_samples is even to allow 50/50 split
    if num_samples % 2 != 0:
        raise ValueError("Number of samples must be even to ensure 50% distribution of labels.")

    # Split the data into two groups: one for each class
    class_0 = df[df[target_column] == 0]
    class_1 = df[df[target_column] == 1]

    # Find the minimum number of samples between the two classes to avoid imbalance issues
    min_class_size = min(len(class_0), len(class_1))

    if min_class_size * 2 < num_samples:
        raise ValueError(f"Not enough data to create a balanced dataset of {num_samples} samples.")

    # Sample from each class to ensure 50/50 split
    half_samples = num_samples // 2
    class_0_sample = class_0.sample(n=half_samples)
    class_1_sample = class_1.sample(n=half_samples)

    # Concatenate the two samples to form the balanced dataset
    df_n = pd.concat([class_0_sample, class_1_sample]).sample(frac=1).reset_index(drop=True)  # Shuffle and reset index

    return df_n

# Usage
y = df[target]
X = df.drop(columns=[target])

# Create balanced sample
df_n = create_balanced_sample(df)
print(df_n.shape)

print(df.isna().sum())

#df.dropna(axis=1, inplace=True)
df_clean = df.dropna(axis=1)
#print(df.isna().sum())

y = df_clean[target]
X = df_clean.drop(columns=[target])
#correlation_matrix = df_clean.corr()

#print("X shape = ", X.shape)
#print("Y shape = ", y.shape)
#y.value_counts()

# Plot the correlation matrix using seaborn
#plt.figure(figsize=(15, 8))
#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
#plt.title('Correlation Matrix')
#plt.show()

#class_correlations = correlation_matrix[target].sort_values(ascending=False)
#all_list_atributs = correlation_matrix[target].sort_values(ascending=False).index.tolist()
#list_atributs = all_list_atributs[1:]

# Display the sorted correlations
#print(class_correlations)
#print(list_atributs)

class ClaMPDataset(): # 4 features -> most corelated atributs
  def __init__(self, target, cut = 0):
    self.target = target
    self.X, self.y, self.correlation_matrix = self.read_csv(cut)
    self.list_atributs = self.list_atributs_corralation()

  def list_atributs_corralation(self):
    # Unstack the correlation matrix to get pair-wise correlations as a series
    correlation_series = self.correlation_matrix.unstack()
    # Convert the series to a DataFrame for better manipulation
    correlation_df = pd.DataFrame(correlation_series, columns=['correlation']).reset_index()
    # Rename the columns for clarity
    correlation_df.columns = ['attribute_1', 'attribute_2', 'correlation']
    # Filter out self-correlations (where correlation == 1)
    correlation_df = correlation_df[correlation_df['attribute_1'] != correlation_df['attribute_2']]
    # Sort the correlation values from highest to lowest
    sorted_correlation_df = correlation_df.sort_values(by='correlation', ascending=False)
    # List to store the final results without repeated attributes
    top_correlations = []
    list_atributs = []
    used_attributes = set()
    for _, row in sorted_correlation_df.iterrows():
        if len(top_correlations) >= 8:
            break
        attr1, attr2, corr_value = row['attribute_1'], row['attribute_2'], row['correlation']
        if attr1 not in used_attributes and attr2 not in used_attributes:
            top_correlations.append((attr1, attr2, corr_value))
            list_atributs.append(attr1)
            list_atributs.append(attr2)
            used_attributes.add(attr1)
            used_attributes.add(attr2)
    return list_atributs

  def read_csv(self, cut):
    # Define the dataset path
    dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"
    # List files in the directory to find the CSV file
    files = os.listdir(dataset_path)
    csv_files = [f for f in files if f.endswith('.csv')]

    # Load the first CSV file (assuming there's only one)
    if csv_files:
        df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
        #print("CSV found and send to df")  # Display first few rows
    else:
        print("No CSV file found in the dataset directory.")
    #df.dropna(axis=1, inplace=True)
    df = df.dropna(axis=1)
    df = self.create_cut(df, cut)
    y = df[self.target]
    X = df.drop(columns=[self.target])
    X = X.apply(pd.to_numeric,errors='coerce')
    correlation_matrix = X.corr()
    return X,y,correlation_matrix

  def create_cut(self, df, cut=0):
    if cut == 0:
        return df

    # Select the rows where Class is 0 and 1
    class_0 = df[df[self.target] == 0]
    class_1 = df[df[self.target] == 1]

    # Select based on the condition of cut % 2 == 0 or cut % 2 == 1
    if cut % 2 == 0:
        df_n = pd.concat([class_0[:cut//2], class_1[:cut//2]])
    else:
        df_n = pd.concat([class_0[cut//2:], class_1[cut//2:]])

    return df_n

  def plot_correlation_matrix(self):
    # Plot the correlation matrix using seaborn
    plt.figure(figsize=(15, 8))
    sns.heatmap(self.correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title('Correlation Matrix')
    plt.show()
  def dataset(self, dimension):
    list_atributs = self.list_atributs
    X_dim = self.X[list_atributs[:dimension]]
    X_train, X_test, y_train, y_test = train_test_split(X_dim, self.y, test_size=0.2, random_state=42)
    #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
    return X_train, X_test, y_train, y_test

import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

class ClaMPDatasetGPT(): # 4 features -> most and least correlated atributs
    def __init__(self, target, cut=0):
        self.target = target
        self.X, self.y, self.correlation_matrix = self.read_csv(cut)
        self.list_atributs = self.list_atributs_correlation()

    def list_atributs_correlation(self):
        # Calculate the correlation of each feature with the target variable
        target_correlation = self.X.corrwith(self.y).abs()

        # Sort the features based on their absolute correlation with the target
        sorted_correlation = target_correlation.sort_values(ascending=False)

        # Select the top n features with the highest correlation
        top_n = 15  # You can adjust this number as needed
        top_correlated = sorted_correlation.head(top_n).index.tolist()

        # Select the top n features with the lowest correlation
        bottom_correlated = sorted_correlation.tail(top_n).index.tolist()

        # Combine the lists
        list_atributs = top_correlated + bottom_correlated

        return list_atributs

    def read_csv(self, cut):
        # Define the dataset path
        dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"

        # List files in the directory to find the CSV file
        files = os.listdir(dataset_path)
        csv_files = [f for f in files if f.endswith('.csv')]

        # Load the first CSV file (assuming there's only one)
        if csv_files:
            df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
            #print("CSV found and send to df")  # Display first few rows
        else:
            print("No CSV file found in the dataset directory.")

        # Drop columns with NaN values
        df = df.dropna(axis=1)

        # Apply the cut if specified
        df = self.create_cut(df, cut)

        # Separate features and target
        y = df[self.target]
        X = df.drop(columns=[self.target])
        X = X.dropna()
        # Calculate the correlation matrix
        correlation_matrix = X.corr()

        return X, y, correlation_matrix

    def create_cut(self, df, cut=0):
        if cut == 0:
            return df

        # Select the rows where Class is 0 and 1
        class_0 = df[df[self.target] == 0]
        class_1 = df[df[self.target] == 1]

        # Select based on the condition of cut % 2 == 0 or cut % 2 == 1
        if cut % 2 == 0:
            df_n = pd.concat([class_0[:cut//2], class_1[:cut//2]])
        else:
            df_n = pd.concat([class_0[cut//2:], class_1[cut//2:]])

        return df_n

    def plot_correlation_matrix(self):
        # Plot the correlation matrix using seaborn
        plt.figure(figsize=(15, 8))
        sns.heatmap(self.correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('Correlation Matrix')
        plt.show()

    def dataset(self, dimension):
        list_atributs = self.list_atributs
        X_dim = self.X[list_atributs[:dimension]]
        X_train, X_test, y_train, y_test = train_test_split(X_dim, self.y, test_size=0.2, random_state=42)
        #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
        return X_train, X_test, y_train, y_test

"""# Quantum Backends

"""

def backends(print_backend=False):
  # Use this command if you didn't save your credentials:
  service = QiskitRuntimeService(channel="ibm_quantum", token="482cfbd6dec57e562dd79640807643b60b7388db6a0ff63e30b4bd92d4d55601a902c6f005413b8a69f6677daf23c82fa14626594ab3656d9c8c0e9229f920de")
  # Load saved credentials
  all = service.backends()
  quantum_backend = ""
  q_hardwares = []
  for i in range(0,len(all)):
    quantum_backend = str(all[i])
    if "<IBMBackend('ibm" in quantum_backend:
      quantum_backend = re.search(r"'(.*?)'", quantum_backend).group(1)  # Extract the text within single quotes
      q_hardwares.append(quantum_backend)
      if (print_backend==True):
        print("Quantum backend:", quantum_backend)
      #break
  return q_hardwares, service

"""## Metrics of Evaluation

We should ahve metrics of evaluation to the Quantum Machine Learning and also the Quantum Hardware with the [backend atributs](https://docs.quantum.ibm.com/api/qiskit-ibm-runtime/qiskit_ibm_runtime.IBMBackend). So with this atribust we will be able to characterize the quantum computers.
"""

def get_confusion_matrix_elements(test_labels, predictions):
    TP = np.sum((test_labels == 1) & (predictions == 1))
    TN = np.sum((test_labels == 0) & (predictions == 0))
    FP = np.sum((test_labels == 0) & (predictions == 1))
    FN = np.sum((test_labels == 1) & (predictions == 0))
    return TP, TN, FP, FN

def mean_qubits(backend, property):
  mean = 0
  for i in range(0,backend.num_qubits):
    if(property == 'readout_error'):
      mean += backend.properties().readout_error(i)
    elif(property=='t1'):
      mean += backend.properties().t2(0)
    elif(property=='t2'):
      mean += backend.properties().t2(0)
  mean=mean/backend.num_qubits
  return mean

def json_qiskit(name):
  TOKEN = "482cfbd6dec57e562dd79640807643b60b7388db6a0ff63e30b4bd92d4d55601a902c6f005413b8a69f6677daf23c82fa14626594ab3656d9c8c0e9229f920de"

  response = requests.request(
    "GET",
    "https://api.quantum-computing.ibm.com/runtime/workloads/me",
    headers={
        "Accept": "application/json",
        "Authorization": "Bearer "+TOKEN
    },
    )
  # Parse the JSON response
  usage_seconds = 0
  estimated_running_time_seconds = 0
  data = response.json()
  # Iterate over the 'workloads' list to find the latest backend with the name
  for workload in data['workloads']:
    if workload['backend'] == name:
          usage_seconds = workload.get('usage_seconds', None)
          estimated_running_time_seconds = workload.get('estimated_running_time_seconds', None)
          break
    else:
        #print(f"Backend {name} not found in the workloads.")
        continue
  return usage_seconds, estimated_running_time_seconds

def calculate_h_m_s_ms(end, start):
  # Calculate elapsed time in seconds
  elapsed_time_seconds = end - start
  # Convert elapsed time into hours, minutes, seconds
  hours = int(elapsed_time_seconds // 3600)
  minutes = int((elapsed_time_seconds % 3600) // 60)
  seconds = int(elapsed_time_seconds % 60)
  milliseconds = int((elapsed_time_seconds - int(elapsed_time_seconds)) * 1000)
  return elapsed_time_seconds#[hours, minutes, seconds, milliseconds]

# Define evaluation metric functions
def calculate_metrics(test_labels, predictions):
    TP, TN, FP, FN = get_confusion_matrix_elements(test_labels, predictions)
    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0 #recall=sensitivity
    sensitivity = recall
    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
    # calculate sensitivity and specificity
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score

def calculate_metrics_hardware(backend):
  #Read the json according to the backend you are using and get the: 'usage_seconds', 'estimated_running_time_seconds'
  usage_seconds, estimated_running_time_seconds = json_qiskit(backend.name)
  mean_readout_error = mean_qubits(backend,'readout_error')
  mean_t1 = mean_qubits(backend,'t1')
  mean_t2 = mean_qubits(backend,'t1')

  return usage_seconds, estimated_running_time_seconds,mean_readout_error, mean_t1, mean_t2

import pandas as pd

# Create an empty DataFrame with specified columns
df_results = pd.DataFrame({
    'Model': [],
    'TP': [],
    'TN': [],
    'FP': [],
    'FN': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': [],
    'Elapsed Time (s)':[],
    'Usage (s)':[],
    'Estimated Usage (s)': [],
    'Num Qubits': [],
    'Median T1':[],
    'Median T2':[],
    'Median Read Out Error':[]
})
df_results

def inicial_df():
  # Create a DataFrame to store the results
  # df_results = pd.DataFrame({
  #     'Model', 'TP', 'TN', 'FP', 'FN', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Elapsed Time (s)', 'Num Qubits', 'Weights', 'Num Classes'
  # })
  df_results = pd.DataFrame({
    'Hardware': [],
    'Dimension': [],
    'TP': [],
    'TN': [],
    'FP': [],
    'FN': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': [],
    'Elapsed Time (s)':[],
    'Usage (s)':[],
    'Estimated Usage (s)': [],
    'Num Qubits': [],
    'Median T1':[],
    'Median T2':[],
    'Median Read Out Error':[]
  })
  return df_results

def metrics_of_evaluation(name, dimension, qsvc,end,start,test_features, test_labels, backend):
  usage_seconds, estimated_running_time_seconds,mean_readout_error, mean_t1, mean_t2 = calculate_metrics_hardware(backend)
  #hours,minutes, seconds,milliseconds = calculate_h_m_s_ms(end, start)
  enlapse_time = end-start
  predictions = qsvc.predict(test_features)
  TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score = calculate_metrics(test_labels, predictions)

  # Create a DataFrame to store the results
  df_results = pd.DataFrame({
      'Hardware':name,
      'Dimension':dimension,
      'TP': [TP],
      'TN': [TN],
      'FP': [FP],
      'FN': [FN],
      'Accuracy': [accuracy],
      'Precision': [precision],
      'Sensitivity': [sensitivity],
      'Specificity': [specificity],
      'F1 Score': [f1_score],
      'Elapsed Time (s)': [enlapse_time],
      'Usage (s)':[usage_seconds],
      'Estimated Usage (s)': [estimated_running_time_seconds],
      'Num Qubits': [backend.num_qubits],
      'Median T1':[mean_t1],
      'Median T2':[mean_t2],
      'Median Read Out Error':[mean_readout_error]

  })
  return df_results

def metrics_of_evaluation_classicaly(svc,dimension,end,start,test_features, test_labels):
  #usage_seconds, estimated_running_time_seconds,mean_readout_error, mean_t1, mean_t2 = calculate_metrics_hardware(backend)
  #hours,minutes, seconds,milliseconds = calculate_h_m_s_ms(end, start)

  predictions = svc.predict(test_features)
  TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score = calculate_metrics(test_labels, predictions)
  usage_time = end-start
  # Create a DataFrame to store the results
  df_results = pd.DataFrame({
      'Dimension':dimension,
      'TP': [TP],
      'TN': [TN],
      'FP': [FP],
      'FN': [FN],
      'Accuracy': [accuracy],
      'Precision': [precision],
      'Sensitivity': [sensitivity],
      'Specificity': [specificity],
      'F1 Score': [f1_score],
      'Elapsed Time (s)': [usage_time],
      'Usage (s)':[usage_time],


  })
  return df_results

"""# QSVM"""

def qsvc_with_ibm_hardware(name, X_train, y_train, X_test, y_test, dimension, service):
  # Get backend
  backend = service.backend(name)
  # Sampler setup
  sampler = Sampler(backend)
  #fidelity = ComputeUncompute(sampler=sampler)

  #QSVC
  num_features = dimension#X_train.size
  feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)
  ansatz = RealAmplitudes(num_qubits=num_features, reps=3)
  MAXITER = 10
  optimizer = COBYLA(maxiter=MAXITER)

  # Quantum Kernel setup
  #quantum_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)
  quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)

  # Transpile quantum kernel for the local backend
  feature_map_compiled = transpile(feature_map, backend=backend)

  # Quantum Kernel setup
  number_qubits = 127 if name != 'ibm_torino' else 133
  quantum_kernel_circuit = QuantumCircuit(number_qubits)#num_features+1)
  quantum_kernel_circuit.append(feature_map_compiled, range(number_qubits))#num_features+1))#range(num_features))

  # Use QuantumCircuit as part of FidelityQuantumKernel
  fidelity_quantum_kernel = FidelityQuantumKernel()
  fidelity_quantum_kernel._quantum_circuit = quantum_kernel_circuit

  # Classification with QSVC using the quantum kernel circuit
  qsvc = QSVC(quantum_kernel=fidelity_quantum_kernel)
  # Train QSVC
  qsvc.fit(X_train, y_train)
  # Test QSVC
  score = qsvc.score(X_test, y_test)
  #print(f"Callable kernel classification test score: {score}")
  return qsvc, backend

def main_qsvc_update_correlation(computers='all', start_at=0, end_at=20, dataset=0):
    df_results = inicial_df()
    if dataset == 0:
        malware = ClaMPDataset(target='class', cut=500)
        results_file = "qsvc_results_correlation_10_10.csv"
    else:
        malware = ClaMPDatasetGPT(target='class', cut=500)
        results_file = "qsvc_results_correlation_10_0.csv"

    if computers != 'all':
        _, service = backends()
        q_hardwares = [computers, computers]
    else:
        q_hardwares, service = backends()

    print("QSVM with high high correlation: ")
    # Check if the file exists, if so, load existing data
    if os.path.exists(results_file):
        df_results = pd.read_csv(results_file)
    #print("here")
    for dimension in range(start_at, end_at):  # having samples > 10 will make kernel crash (December 2024)
        X_train, X_test, y_train, y_test = malware.dataset(dimension)
        print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

        for i in range(0, len(q_hardwares) - 1):
            q_hardware = q_hardwares[i]
            start = time.time()
            qsvc, backend = qsvc_with_ibm_hardware(q_hardware, X_train, y_train, X_test, y_test, dimension, service)
            df_model = metrics_of_evaluation(q_hardware, dimension, qsvc, time.time(), start, X_test, y_test, backend)

            df_results = pd.concat([df_results, df_model], ignore_index=True)

            # Save updated results to CSV
            df_results.to_csv(results_file, index=False)

            # Print the last updated row
            #print("Last Updated Row:\n", df_results.iloc[-1])

    return df_results

# def main_qsvc_update_correlation_10_0(computers='all', start_at=0, end_at=20):
#     df_results = inicial_df()
#     malware = ClaMPDatasetGPT(target='class', cut=500)

#     if computers != 'all':
#         _, service = backends()
#         q_hardwares = [computers, computers]
#     else:
#         q_hardwares, service = backends()

#     results_file = "qsvc_results_correlation_10_0.csv"

#     # Check if the file exists, if so, load existing data
#     if os.path.exists(results_file):
#         df_results = pd.read_csv(results_file)

#     for dimension in range(start_at, end_at):  # having samples > 10 will make kernel crash (December 2024)
#         X_train, X_test, y_train, y_test = malware.dataset(dimension)
#         print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#         for i in range(0, len(q_hardwares) - 1):
#             q_hardware = q_hardwares[i]
#             start = time.time()
#             qsvc, backend = qsvc_with_ibm_hardware(q_hardware, X_train, y_train, X_test, y_test, dimension, service)
#             df_model = metrics_of_evaluation(q_hardware, dimension, qsvc, time.time(), start, X_test, y_test, backend)

#             df_results = pd.concat([df_results, df_model], ignore_index=True)

#             # Save updated results to CSV
#             df_results.to_csv(results_file, index=False)

#             # Print the last updated row
#             #print("Last Updated Row:\n", df_results.iloc[-1])

#     return df_results

from sklearn.pipeline import make_pipeline

def main_svc_correlation(dataset=0):
  df_results = inicial_df()
  if dataset == 0:
    malware = ClaMPDataset(target='class', cut=500)
    results_file = "svc_results_correlation_10_10.csv"
  else:
    malware = ClaMPDatasetGPT(target='class', cut=500)
    results_file = "svc_results_correlation_10_0.csv"
  for dimension in range(1,27):
    X_train, X_test, y_train, y_test = malware.dataset(dimension)
    print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)
    start = time.time()
    svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))
    svc.fit(X_train, y_train)
    df_model = metrics_of_evaluation_classicaly(svc,dimension,time.time(),start,X_test, y_test)

    df_results = pd.concat([df_results,df_model], ignore_index=True)
    # print(df_results)
    df_results.to_csv(results_file, index=False)
  return df_results

# def main_svc_correlation_10_0():
#   df_results = inicial_df()
#   malware = ClaMPDatasetGPT(target='class', cut=500)
#   results_file = "svc_results_correlation_10_0.csv"
#   for dimension in range(1,27):
#     X_train, X_test, y_train, y_test = malware.dataset(dimension)
#     print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)
#     start = time.time()
#     svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))
#     svc.fit(X_train, y_train)
#     df_model = metrics_of_evaluation_classicaly(svc,dimension,time.time(),start,X_test, y_test)

#     df_results = pd.concat([df_results,df_model], ignore_index=True)
#     # print(df_results)
#     df_results.to_csv(results_file, index=False)
#   return df_results

def svc_qkernel(name, X_train, y_train, X_test, y_test, dimension, service):
    backend = service.backend(name)
    sampler = Sampler(backend)

    # QSVC setup
    num_features = dimension  # Number of features
    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)
    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)

    # Quantum Kernel setup
    quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)

    # Transpile quantum kernel for the backend
    feature_map_compiled = transpile(feature_map, backend=backend)

    # Evaluate the quantum kernel matrices
    X_matrix_train = quantum_kernel.evaluate(x_vec=X_train)
    X_matrix_test = quantum_kernel.evaluate(x_vec=X_test, y_vec=X_train)

    # # Ensure y_train is 1D
    # if y_train.ndim > 1:
    #     y_train = y_train.flatten()  # Flatten y_train to make it 1D

    # SVC model with precomputed kernel
    svc_qkernel_model = SVC(kernel="precomputed")
    print("Kernel shape:", X_matrix_train.shape)
    svc_qkernel_model.fit(X_matrix_train, y_train)

    # Evaluate the model
    adhoc_score_precomputed_kernel = svc_qkernel_model.score(X_matrix_test, y_test)
    print(f"Precomputed kernel classification test score: {adhoc_score_precomputed_kernel}")

    return svc_qkernel_model, backend, quantum_kernel, X_matrix_test

def main_svc_qkernel_correlation(dataset=0, computers="ibm_brisbane"):
    df_results = inicial_df()
    if dataset == 0:
        malware = ClaMPDataset(target='class', cut=500)
        results_file = "svc_qkernel_results_correlation_10_10.csv"
    else:
        malware = ClaMPDatasetGPT(target='class', cut=500)
        results_file = "svc_qkernel_results_correlation_10_0.csv"

    if computers != 'all':
        _, service = backends()
        q_hardwares = [computers, computers]
    else:
        q_hardwares, service = backends()

    for dimension in range(2, 27):
        X_train, X_test, y_train, y_test = malware.dataset(dimension)
        print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

        # # Ensure y_train and y_test are 1D
        # if y_train.ndim > 1:
        #     y_train = y_train.flatten()
        # if y_test.ndim > 1:
        #     y_test = y_test.flatten()

        start = time.time()
        svc, backend, quantum_kernel, X_matrix_test = svc_qkernel(computers, X_train, y_train, X_test, y_test, dimension, service)

        # # Compute the kernel matrix for prediction
        # adhoc_matrix_test = quantum_kernel.evaluate(x_vec=X_test, y_vec=X_train)

        # # Predict using the precomputed kernel matrix
        # predictions = svc.predict(adhoc_matrix_test)

        df_model = metrics_of_evaluation_classicaly(svc, dimension, time.time(), start, X_matrix_test, y_test)

        df_results = pd.concat([df_results, df_model], ignore_index=True)
        df_results.to_csv(results_file, index=False)

    return df_results

"""### SVC + Quantum Kernel"""
#need to add a loop here
df_result_qkernel0 = main_svc_qkernel_correlation(0)
#df_result_qkernel
print("**********************************************")
df_result_qkernel1 = main_svc_qkernel_correlation(1)
#df_result_qkernel

"""### Quantum Model Training

* Fist is the dataset with highest corelations
* The problem is that we are doing feature selection based on the best features that are more corelated with our output. But we want to have features that are now as good to our output. Like the best corelated and the least corelated for example
"""

df_result_qsvc0 = main_qsvc_update_correlation("ibm_brisbane", start_at=12, end_at=28, dataset=0) #9-20
df_result_qsvc0 = df_result_qsvc.dropna(axis=1)
#df_result_qsvc
print("**********************************************")
df_result_qsvc1 = main_qsvc_update_correlation("ibm_brisbane", start_at=12, end_at=28, dataset=1) #9-20
df_result_qsvc1 = df_result_qsvc.dropna(axis=1)
# df_result_qsvc

"""### Classic Model Training:"""

df_result_svc0 = main_svc_correlation(0) #"ibm_brisbane", start_at=2, end_at=20)
# df_result_svc0

df_result_svc1 = main_svc_correlation(1) #"ibm_brisbane", start_at=2, end_at=20)
# df_result_svc1

"""## Export answer in external file

ML in classic computer.
"""

# file_path = "/../results/df_result_classic.csv"
# df_classic.to_csv(file_path, index=False)

# Copy the original DataFrame
formatted_df = df_classic.copy()

# Identify the maximum values for the specified columns
columns_to_bold = ['TP', 'TN', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score', 'Elapsed Time (s)']
max_values = formatted_df[columns_to_bold].max()

# Identify the minimum usage
usage_column = 'Usage (s)'  # Replace with the actual column name for usage
min_usage = formatted_df[usage_column].min()

# Round specified columns to 1 decimal place
columns_to_round = ['TP', 'TN', 'FP', 'FN']
formatted_df[columns_to_round] = formatted_df[columns_to_round].astype(float).round(1)

# Apply bold formatting to the maximum values
for col in columns_to_bold:
    formatted_df[col] = formatted_df[col].apply(lambda x: f"\\textbf{{{x}}}" if x == max_values[col] else x)

# Highlight the minimum usage value
formatted_df[usage_column] = formatted_df[usage_column].apply(
    lambda x: f"\\textbf{{{x}}}" if x == min_usage else x
)

# Generate the LaTeX table
latex_output = formatted_df.to_latex(
    index=False,
    escape=False,
    caption="Performance analysis of classification models with highlighted maximum values and minimum usage:",
    label="tab:classification_performance"
)

# Print the LaTeX table
print(latex_output)

"""Hipothesis:
- The results might be different
- Superconducting qubit with

- Get the time for predicting the test data
"""
